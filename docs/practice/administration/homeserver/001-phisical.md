# Физическое устройство инфраструктуры

В начале работы я выделил уголок для будущей стойки.
<img src="/img/begining_in_corner.jpg" width="300">
В дальнейшем я сделаю сюда сварную стойку с выдвижными полками. Я не буду закупать какое-то дорогое серверное оборудование. Первое время обойдусь обычными старенькими компьютерами и рабочей станцией. И дальше базироваться буду на простых ПК, так как для них намного проще найти более дешёвые комплектующие. На данный момент мой парк из ПК-шечек выглядит так:

<img src="/img/my_pcs_in_begining.jpg" width="600">

Самый дальний будет использоваться под сервер виртуализации (**node1**), средний - под NAS (**strg0**), и третий под сервер баз данных (**spec0**).

Их харакетристики:

| _Характеристики_          | node1 | strg0             | spec0                    |
| :------------------------ | :---- | :---------------- | :----------------------- |
| **BIOS**                  |       |                   | American Megatrends Inc. |
| **Процессор**             |       |                   |                          |
| Название                  |       |                   |                          |
| Количество ядер / потоков |       |                   |                          |
| Тактовая частота          |       |                   |                          |
| **ОЗУ**                   |       |                   |                          |
| Название / объём          |       |                   |                          |
| Количество планок         |       |                   |                          |
| Частота                   |       |                   |                          |
| **ПЗУ**                   |       |                   |                          |
| Название / объём          |       |                   |                          |
| Количество устройств      |       |                   |                          |
| **Сетевые интерфейсы**    |       |                   |                          |
| Скорость / IP             |       | / `192.168.0.200` |                          |

### Сетевые устройства

На границе с провайдером у меня стоит маршрутизатор **TP-LINK Archer AX50**. Имеется также три коммутатора: управляемый **TP-LINK TL-SG105E** и неуправляемые **TP-LINK TL-SG105**, **TP-LINK LS1008G**. Соответственно, последняя цифра в маркировке соответствует количеству портов. Все же они работают в дуплексном режиме гигабит на порт.

## Создание загрузочной флэшки

Когда я использовал Windows, то для создания загрузочных флешек чаще всего использовал программу **Rufus**. При переходе на Linux пару раз пробовал записывать образы программой `dd`.

Все эти способы обладают одним важным недостатком - они делают из флешки на несколько гигабайт устройство, которое хранит только одну операционку. Для решения этой проблемы была создана программа **Ventoy**. После того, как она установлена на флешку, достаточно просто закидывать файлы образов в определёный каталог и идти к физической машине для устновки этого образа.

### Установка Ventoy на Ubuntu

Скачиваем арххив [здесь](https://github.com/ventoy/Ventoy/releases)

Затем распаковываем архив и переходим в каталог:

```shell
tar -xf ventoy-1.0.88-linux.tar.gz
cd ventoy-1.0.88
```

Запускаем от рута веб морду и видим:

```shell
sudo ./VentoyWeb.sh

## out >>>>>>
===============================================================
  Ventoy Server 1.0.88 is running ...
  Please open your browser and visit http://127.0.0.1:24680
===============================================================

################## Press Ctrl + C to exit #####################

```

<img src="/img/Ventoy-web-notinstall.png" width="600">

После перехода по ссылке, мы увидим наши флешки в выпадающем списке устройств. В случае, если на флешке уже имеется установленная версия **Ventoy**, то скрипт предложит обновить версию:

<img src="/img/Ventoy-web.png" width="600">

В настройках можно выбрать стиль разметки (MBR, GPT), выделить часть устройства в отдельный раздел, удалить **Ventoy**. Мы же нажимаем кнопку _Установить_ и соглашаясь на <span style="color: red;">форматирование устройства</span>, устанавливаем программу.

Флешка автоматически подмонтировалась и открылся каталог, куда и можно закидывать образы.

Также в изначально распакованном каталоге имеется скрипт `VentoyPlugson.sh`, который запускается следующим синтаксисом:

```shell
sudo bash VentoyPlugson.sh /dev/sdb1
```

где в качестве устройства подразумевается то, на котором установлен **Ventoy**. Именно устройство, а не точка монтирования!

Опять же будет запущена веб морда, в которой можно произвести тонкую настройку программы вплоть до внешнего вида, настроек безопасности, вывода меню и прочее. Мне не столь важна эта функциональность, поэтому я не стал разбираться в нюансах.

### Образы для загрузочной флешки

На данный момент я располагаю флешкой на 16 Gb, однако я уже подобрал флешку на 128 Gb. Исходя из этого размера я помещу на неё следующий список образов:

| Образ               |                           Ссылка на источник                           | Комментарий                                              |
| :------------------ | :--------------------------------------------------------------------: | :------------------------------------------------------- |
| Proxmox             | [ссылка](https://www.proxmox.com/de/downloads/category/iso-images-pve) | Я скачиваю последнюю версию через торрент                |
| OpenMediaVault      |          [ссылка](https://www.openmediavault.org/?page_id=77)          | Тут выбираем версию и скачиваем файл через _SourceForge_ |
| Ubuntu Server 22.04 |                                  []()                                  |
| Debian              |                                  []()                                  |
| Xubuntu 22.04       |                                  []()                                  |

Очень важно иметь запасные исгнальные провода для корректной работы мониторов.

## Установка и настройка Proxmox

Proxmox - вещь крайне простая и интуитивная, но не когда дело касается работы с дисками. Возможно я не учитываю какую-то возможность более тонкой настройки распределения дискового пространства при установке, но опятным путём я пришёл к выводу, что наиболее эффективным способом распределить дисковое пространство по моемужеланию будет следующая последовательность.

В Proxmox по умолчанию применяются два хранилища:
№ |ID в Proxmox| _lsblk_ | Предназначение
:--:|:--|:--|:--
1 | **_local-lvm_** | `/dev/pve/data` | rootdir, images
2 | **_local_** | `/dev/pve/root` | iso,vztmpl,backup

Я хочу, чтобы у меня было одно хранилище для всего и чтобы оно было свободно, то есть мне не нужно отображение занимаемого гипервизором пространства в веб морде. Допустим, у нас есть жёсткий диск на 76G. Поэтому делаем так:

При установке Proxmox в момент выбора носителя, нажимаем _Options_ и задаём максимум на _root_ - 15G, на _swap_ - 0, минимальное свободное пространство - 37G.

### Лечим обновления

Теперь можно обновить пакеты, но сначала нужно подключить бесплатные репозитории. Редактируем файл `/etc/apt/sources.list.d/pve-enterprise.list` следующим образом:

```shell
echo -e "# deb https://enterprise.proxmox.com/debian/pve bullseye pve-enterprise\ndeb http://download.proxmox.com/debian/pve buster pve-no-subscription" > /etc/apt/sources.list.d/pve-enterprise.list
```

И обновляемся `apt update && apt upgrade`

### Предварительная настройка

- Устанавливаем следующие пакеты:
  `apt install -y vim`
- Изменяем назначение хранилищ `/etc/pve/storage.cfg`.

  Изначально он выглядит так:

  ```shell
  dir: local
          path /var/lib/vz
          content iso,vztmpl,backup

  lvmthin: local-lvm
          thinpool data
          vgname pve
          content rootdir,images
  ```

Мы же оставляем его в таком виде:

```shell
    dir: local
            path /var/lib/vz
            maxfiles 0
            content backup,iso,vztmpl,rootdir,images
```

- Пересоздаём примонтированный к `/var/lib/vz` логический том:

  ```shell
  ## мониторим логические тома
  lvs
  # out >>>>
  LV   VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  data pve twi-a-tz-- 20.02g             0.00   1.58
  root pve -wi-ao---- 15.00g

  ## удаляем том data из группы томов pve
  lvremove pve/data

  ## создаём том data в группе томов pve
  lvcreate --name data -l +100%FREE pve

  ## форматируем новый том под ext4
  mkfs.ext4 /dev/pve/data
  ```

- Добавляем новый том в **fstab**: `/dev/pve/data /var/lib/vz ext4 defaults 0 1`
- Перезагружаемся

Теперь я добился того, чего хотел в отношении дискового пространства своего сервера.

### Машины на локальном хранилище сервера виртуализации

Так как я пока не планирую ставить второй сервер и формировать кластер, то для экономии энергии я установлю сервисные машины на локальном хранилище.

### Создание шаблона Cloud Init

Для более быстрого развёртывания виртуальных машин сделаем cloud init образа шаблон. В машины, развёрнутые через такие шаблоны можно зайти только имея на них ключи SSH на них от машины, с которой производится вход. Поэтому сразу нужно определиться с какой машины будет происходить вход. Будем считать, что эту роль будет выполнять хостовая машина Proxmox - уж она точно никуда не денется. Поэтому зайдём на неё через SSH и будем выполнять все действия с неё.

Порядок такой:

- Создаём машину (по нашей кодификации - это будет VM 9150)
- Настраиваем Cloud Init
- Скачиваем и подготавливаем образ
- Импортируем его в нашу машину

#### Создание машины

Будем создавать машину через веб интерфейс. Самое главное - это не указывыать носитель во вкладке `ОС` (_нет носителя_), поставить галку qemu-agent в системе и удалить предложенный диск.

В созданной машине в **Оборудовании** добавляем диск Cloud Drive, указав хранилище, на котором будет располагаться этот диск. Идя по порядку, в пункте **Cloud-Init** вновь созданой машины, мы задаём имя пользователя, его пароль, импортируем или вводим ключ машины, с которой будет производиться доступ. В конце конфигурируем сеть. Лучше установить настройки по DHCP. После настроки регенерируем образ.

Нужно дополнительно создать последовательный терминал. Для этого нужно ввести следующую команду:

```shell
qm set 9160 --serial0 socket --vga serial0
```

Подробнее о команде `qm` [здесь](https://pve.proxmox.com/pve-docs/qm.1.html)

#### Работа над образом

Сначала скачиваем образ. Допустим, что мы будем устанавливать Debian 11. По запросу "**cloud-images debian**" находим следующую ссылку.

```shell
wget https://cloud.debian.org/images/cloud/bullseye/latest/debian-11-genericcloud-amd64.qcow2
```

Допустим мы скачали файл с расширением `*.img`, тогда нужно пересохранить файл в `*.qcow2`:

```shell
mv file.img file.qcow2
```

Затем нужно подготовить и импортировать диск в нашу машину. Меняем номинальный размер диска. Потом, собственно, импортируем в машину с номером `<..>` диск машины `<..>` в хранилище `<..>`.

```shell
qemu-img resize debian-11-genericcloud-amd64.qcow2 16G
qm importdisk 9160 debian-11-genericcloud-amd64.qcow2 local
```

В списке оборудования появился неиспользуемый жёсткий диск. В его настройках ставим галку на `отклонить`, снимаем галку с `IO thread` и в расширенных настройках включаем эмуляцию SSD. В опциях ('параметрах') в пункте **порядок загрузки** наш диск перетаскиваем на второе место в очереди на загрузку. На данном этапе следует конвертировать машину в шаблон.

#### Клонирование и настройка новой машины

Теперь можно клонировать машину. Вводим индекс, даём имя, которое, кстати, станет именем машины. Запускаем и ждем пару минут даже после того, как выскочит приглашение для ввода логина и пароля. В машине устанавливаем `qemu-guest-agent`. Чтобы скорректировать сетевые настройки, нужно отредактировать файл `/etc/netplan/<settings_file>.yaml` в Ubuntu и `/etc/network/interfaces` в Debian.

**_/etc/netplan/<settings_file>.yaml_**

```yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses:
        - 192.168.3.151/21
      gateway4: 192.168.0.1
      match:
        macaddress: 5e:db:e9:75:d1:8d
      nameservers:
        addresses:
          - 192.168.0.1
        search:
          - km468
      set-name: eth0
```

**_/etc/network/interfaces_**

```text
auto lo
iface lo inet loopback

auto enp3s0
iface enp3s0 inet static
address 192.168.0.200
gateway 192.168.0.1
netmask 255.255.248.0
```

### Сертификаты

При переходе на домен `https://domain.dm:8006` мы можем лицезреть реакцию любого уважающего себя браузера на самоподписанный сертификат от какого бы то ни было сервиса, который с криком "мамой клянусь" утверждает, что он "настоящий" и тому, что видит польователь на экране можно доверять на все сто процентов. Короче, не доверяет браузер этим вашим... Вследствие чего и пишет "<span style="color: red;">Не защищено</span>"

Короче, здсь мы немного забежим вперёд, но представим, что у нас есть локальный центр сертификации или того веселее, имеется белый статический IP. В первом случае мы бы использовали сервис по типу Let's Encrypt, но во втором мы сами себе Let's Encrypt, поэтому разберёмся с сертификатами в PVE.

<span style="color: red;">Важно!!!</span> Все эти манипуляции, особенно в первый раз я рекомендую сделать на только что установленной системе PVE, потому что мои эксперименты привели к печальным последствиям - мой сетификат слетел, я попытался исправить ситуацию, потом посмотрел как это делается и в итоге ситуация оказалась настолько печальной, что при отсутствии опыта у меня появился только один вариант решения проблемы: переустановить PVE.

Итак, первый вопрос: **где?**

## Установка и настройки OpenMediaVault

В целях разделения физического расположения сервисных и прикладных машин, я подключу NAS сервер к моей системае. Дело в том, что прикладные машины по своей специфике нужны далеко не всегда, а место, которое они будут занимать требует подключения нескольких террабайт. Чего стоит один только Nextcloud
